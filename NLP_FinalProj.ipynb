{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''' FAQ Machine\n",
    "\n",
    "FAQ Machine is a Frequently Asked Questions semantic matching application that will produce improved results using NLP features and techniques. This project has implementation of bag-of-words strategy and improved method\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import nltk #Natural language Processing library\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading FAQ data from CSV file\n",
    "data = pd.read_csv(\"BankFAQs10.csv\")\n",
    "\n",
    "#Extracting the questions and answers from the dataframe.\n",
    "question = data['Question'].values\n",
    "answer = data['Answer'].values\n",
    "\n",
    "# Number of results to be displayed\n",
    "num_of_results = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cosine Similarity function\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    int = set(v1.keys()) & set(v2.keys())\n",
    "    num = sum([v1[x] * v2[x] for x in int])\n",
    "\n",
    "    summing1 = sum([v1[x]**2 for x in v1.keys()])\n",
    "    summing2 = sum([v2[x]**2 for x in v2.keys()])\n",
    "    den = math.sqrt(summing1) * math.sqrt(summing2)\n",
    "\n",
    "    if not den:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(num) / den\n",
    "\n",
    "def text2vec(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arunk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arunk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Downloading Stop words data\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#Downloading Punctuations data\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "#Downloading Lancaster Stemmer\n",
    "l_stemmer = LancasterStemmer()\n",
    "\n",
    "#Downloading WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Using Stanford NLP's Stanford Parser for parising\n",
    "#   **NOTE** : Download the two jar files in your local directory before executing the program\n",
    "parser = StanfordParser('stanford-parser-full-2018-02-27/stanford-parser.jar','stanford-english-corenlp-2018-02-27-models.jar')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "Implement a deeper NLP pipeline to extract semantically rich features from the FAQs and Answers\n",
    "o Tokenize the FAQs and Answers into sentences and words\n",
    "o Remove stop-words\n",
    "o Lemmatize the words to extract lemmas as features\n",
    "o Stem the words to extract stemmed words as features\n",
    "o Part-of-speech (POS) tag the words to extract POS tag features\n",
    "o Perform dependency parsing or full-syntactic parsing to parse-tree based patterns as features\n",
    "o Using WordNet, extract hypernymns, hyponyms, meronyms, AND holonyms as\n",
    "features\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 3: Take the Question and Answers from the whole FAQ data and do the below specified functions\n",
    "\n",
    "class FAQ_data:\n",
    "    def __init__(self, str_question, str_answer):\n",
    "        self.all_question = []\n",
    "        self.all_answer = []\n",
    "        self.all = []\n",
    "        self.bow = []\n",
    "        \n",
    "        # Remove the unwanted spaces, tabs and new line character\n",
    "        self.str_question = str_question.strip()\n",
    "        self.str_answer = str_answer.strip()\n",
    "\n",
    "        # Split the strings (words) into tokens based on whitespace and punctuation\n",
    "        # Also called as Bag of Words\n",
    "        self.bag_question = word_tokenize(self.str_question)\n",
    "        self.bag_answer = word_tokenize(self.str_answer)\n",
    "\n",
    "        #Count the number of tokens in the bag\n",
    "        self.counter_question = Counter(self.bag_question)\n",
    "        self.counter_answer = Counter(self.bag_answer)\n",
    "        \n",
    "        # Tag the Question and Answer tokens based on POS in the form of (token, token's POS)\n",
    "        self.bag_question_pos = nltk.pos_tag(self.bag_question)\n",
    "        self.bag_answer_pos = nltk.pos_tag(self.bag_answer)\n",
    "\n",
    "        # Filtering stop words from the bag of tokens\n",
    "        self.bag_question_sw_removed = [word for word in self.bag_question if word.lower() not in stopwords]\n",
    "        self.bag_answer_sw_removed = [word for word in self.bag_answer if word.lower() not in stopwords]\n",
    "        self.all_question +=self.bag_question_sw_removed\n",
    "        self.all_answer+=self.bag_answer_sw_removed\n",
    "\n",
    "        # Reducing each token to its root or base word : Stemming\n",
    "        self.bag_question_stemmed = []\n",
    "        self.bag_answer_stemmed = []\n",
    "        for word in self.bag_question:\n",
    "            self.bag_question_stemmed.append(l_stemmer.stem(word))\n",
    "        for word in self.bag_answer:\n",
    "            self.bag_answer_stemmed.append(l_stemmer.stem(word))\n",
    "            \n",
    "        self.all_question +=self.bag_question_stemmed\n",
    "        self.all_answer+=self.bag_answer_stemmed\n",
    "\n",
    "        # Group tokens based on the words lemma\n",
    "        self.bag_question_lemmatized = []\n",
    "        self.bag_answer_lemmatized = []\n",
    "        for word in self.bag_question:\n",
    "            self.bag_question_lemmatized.append(lemmatizer.lemmatize(word))\n",
    "        for word in self.bag_answer:\n",
    "            self.bag_answer_lemmatized.append(lemmatizer.lemmatize(word))\n",
    "            \n",
    "        self.all_question +=self.bag_question_lemmatized\n",
    "        self.all_answer+=self.bag_answer_lemmatized\n",
    "\n",
    "        # Tree parsing representing the syntactic structure of a based on context-free grammar\n",
    "        self.sent_question = sent_tokenize(self.str_question)\n",
    "        self.sent_answer = sent_tokenize(self.str_answer)\n",
    "        self.parse_tree_q = parser.raw_parse_sents(self.sent_question)\n",
    "        self.parse_tree_a = parser.raw_parse_sents(self.sent_answer)\n",
    "\n",
    "        # Extracting hypernymns, hyponyms, meronyms, holonyms from Wordnet \n",
    "        self.hypernymns = []\n",
    "        self.hyponyms = []\n",
    "        self.meronyms = []\n",
    "        self.holonyms = []\n",
    "        self.bag_counter = Counter(self.bag_question_sw_removed) + Counter(self.bag_answer_sw_removed)\n",
    "        for word in self.bag_counter.keys():\n",
    "            synsets = wn.synsets(word)\n",
    "            if synsets:\n",
    "                max_cos = 0.0\n",
    "                target_synset = None\n",
    "                for synset in synsets:\n",
    "                    definition = synset.definition()\n",
    "                    cos = cosine_similarity(Counter(self.bag_question + self.bag_answer), Counter(definition))\n",
    "                    if cos > max_cos:\n",
    "                        max_cos = cos\n",
    "                        target_synset = synset\n",
    "                if target_synset is None:\n",
    "                    target_synset = synsets[0]\n",
    "                if target_synset.hypernyms():\n",
    "                    self.hypernymns += target_synset.hypernyms()\n",
    "                if target_synset.hyponyms():\n",
    "                    self.hyponyms += target_synset.hyponyms()\n",
    "                if target_synset.part_meronyms():\n",
    "                    self.meronyms += target_synset.part_meronyms()\n",
    "                if target_synset.part_holonyms():\n",
    "                    self.holonyms += target_synset.part_holonyms()\n",
    "        \n",
    "        self.all = self.all_question + self.hypernymns + self.hyponyms + self.meronyms + self.holonyms + self.all_answer \n",
    "        \n",
    "        self.bow = self.bag_question + self.bag_answer \n",
    "        \n",
    "    def print(self):\n",
    "        print(\"Question:\", self.str_question)\n",
    "        print(\"Answer:\", self.str_answer)\n",
    "\n",
    "    def print_bag(self):\n",
    "        print(\"Question:\", self.bag_question)\n",
    "        print(\"Answer:\", self.bag_answer)\n",
    "\n",
    "    def print_counter(self):\n",
    "        print(\"Question:\", self.counter_question)\n",
    "        print(\"Answer:\", self.counter_answer)\n",
    "\n",
    "    def concat(self):\n",
    "        return self.bag_question+self.bag_answer\n",
    "\n",
    "    def print_all_features(self):\n",
    "        print(self.str_question)\n",
    "        print(self.str_answer)\n",
    "        print(\"***** Removing Stop Words *****\")\n",
    "        print('\\t\\t', self.bag_question_sw_removed)\n",
    "        print('\\t\\t', self.bag_answer_sw_removed)\n",
    "        print(\"***** Stemming Words *****\")\n",
    "        print('\\t\\t', self.bag_question_stemmed)\n",
    "        print('\\t\\t', self.bag_answer_stemmed)\n",
    "        print(\"***** Lemmatizing Words *****\")\n",
    "        print('\\t\\t', self.bag_question_lemmatized)\n",
    "        print('\\t\\t', self.bag_answer_lemmatized)\n",
    "        print(\"***** Tagging with Parts of Speech *****\")\n",
    "        print('\\t\\t', self.bag_question_pos)\n",
    "        print('\\t\\t', self.bag_answer_pos)\n",
    "        print(\"***** Creating Parse Tree *****\")\n",
    "        for i in self.parse_tree_q:\n",
    "            for j in i:\n",
    "                print(j)\n",
    "        for i in self.parse_tree_a:\n",
    "            for j in i:\n",
    "                print(j)\n",
    "        print(\"***** Implementing Word net features *****\")\n",
    "        print(\"***** Adding Hypernyms *****\")\n",
    "        print('\\t\\t', self.hypernymns)\n",
    "        print(\"***** Adding Hyponyms *****\")\n",
    "        print('\\t\\t', self.hyponyms)\n",
    "        print(\"***** Adding Meronyms *****\")\n",
    "        print('\\t\\t', self.meronyms)\n",
    "        print(\"***** Adding Holonyms *****\")\n",
    "        print('\\t\\t', self.holonyms)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized successully\n"
     ]
    }
   ],
   "source": [
    "faq_corpus = []\n",
    "for text in range(50):\n",
    "    faq_corpus.append(FAQ_data(question[text], answer[text]))\n",
    "print(\"Tokenized successully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Task4(x):\n",
    "    while x:\n",
    "        user_query = input(\"For help menu: Enter jarvis \\nEnter your question:\\n\")\n",
    "        print(\"********************************************************\")\n",
    "\n",
    "        # Help menu\n",
    "        if user_query == \"jarvis\":\n",
    "            print(\"Manual: \\nFAQ: Show original data \\nBOW: Bag of Words \\nCount: Show the count \\nFeature: Show all the features\")\n",
    "        elif user_query == \"FAQ\":\n",
    "            print(data)\n",
    "        elif user_query == \"show\":\n",
    "            for faq in faq_corpus:\n",
    "                print(faq_corpus.index(faq))\n",
    "                faq.print()\n",
    "        elif user_query == \"BOW\":\n",
    "            for faq in faq_corpus:\n",
    "                print(faq_corpus.index(faq))\n",
    "                faq.print_bag()\n",
    "        elif user_query == \"Count\":\n",
    "            for faq in faq_corpus:\n",
    "                print(faq_corpus.index(faq))\n",
    "                faq.print_counter()\n",
    "        elif user_query == \"Feature\":\n",
    "            for faq in faq_corpus:\n",
    "                print(faq_corpus.index(faq))\n",
    "                faq.print_all_features()\n",
    "        else:\n",
    "            # Remove the unwanted spaces, tabs and new line character\n",
    "            user_input = user_query.strip()\n",
    "\n",
    "            # Split the strings (words) into tokens based on whitespace and punctuation\n",
    "            # Also called as Bag of Words\n",
    "            user_sents = sent_tokenize(user_input)\n",
    "            user_bag = word_tokenize(user_input)\n",
    "\n",
    "            # Tag the Question and Answer tokens based on POS in the form of (token, token's POS)\n",
    "            user_pos = nltk.pos_tag(user_bag) \n",
    "\n",
    "            # Filtering stop words from the bag of tokens\n",
    "            user_sw_removed = [w for w in user_bag if w.lower() not in stopwords]\n",
    "\n",
    "            # Reducing each token to its root or base word : Stemming\n",
    "            user_stemmed = []\n",
    "            user_lemmatized = []\n",
    "            for word in user_bag:\n",
    "                user_stemmed.append(l_stemmer.stem(word))\n",
    "                user_lemmatized.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "            # Tree parsing representing the syntactic structure of a based on context-free grammar\n",
    "            user_tree = parser.raw_parse_sents(user_sents)\n",
    "\n",
    "            # Extracting hypernymns, hyponyms, meronyms, holonyms from Wordnet \n",
    "            user_hypernymns = []\n",
    "            user_hyponyms = []\n",
    "            user_meronyms = []\n",
    "            user_holonyms = []\n",
    "            user_bag_counter = Counter(user_sw_removed)\n",
    "            for word in user_bag_counter.keys():\n",
    "                synsets = wn.synsets(word)\n",
    "                if synsets:\n",
    "                    max_cos = 0.0\n",
    "                    output_synset = None\n",
    "                    for synset in synsets:\n",
    "                        definition = synset.definition()\n",
    "                        cos = cosine_similarity(Counter(user_bag), Counter(definition))\n",
    "                        if cos > max_cos:\n",
    "                            max_cos = cos\n",
    "                            output_synset = synset\n",
    "                    if output_synset is None:\n",
    "                        output_synset = synsets[0]\n",
    "                    if output_synset.hypernyms():\n",
    "                        user_hypernymns += output_synset.hypernyms()\n",
    "                    if output_synset.hyponyms():\n",
    "                        user_hyponyms += output_synset.hyponyms()\n",
    "                    if output_synset.part_meronyms():\n",
    "                        user_meronyms += output_synset.part_meronyms()\n",
    "                    if output_synset.part_holonyms():\n",
    "                        user_holonyms += output_synset.part_holonyms()\n",
    "            # Taking the features into a bag\n",
    "            user_bag += user_hypernymns + user_hyponyms + user_meronyms + user_holonyms + user_stemmed + user_lemmatized\n",
    "            user_counter = Counter(user_bag)\n",
    "            cos_counter = {}\n",
    "            for faq in faq_corpus:\n",
    "                cos = cosine_similarity(user_counter, Counter(faq.all))\n",
    "                cos_counter[faq_corpus.index(faq)] = cos\n",
    "\n",
    "            show_count = 0\n",
    "            for index in sorted(cos_counter, key=cos_counter.get, reverse=True):\n",
    "                if cos_counter[index] > 0 and show_count < num_of_results:\n",
    "                    print(\"*****FAQ Index: \", index, \"\\t*****Cosine Similarity: \", cos_counter[index])\n",
    "                    faq_corpus[index].print()\n",
    "                    print()\n",
    "                    show_count += 1\n",
    "        x-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Task2(x):\n",
    "    while x:\n",
    "        user_query = input(\"For help menu: Enter jarvis \\nEnter your question:\\n\")\n",
    "        print(\"********************************************************\")\n",
    "\n",
    "        # Help menu\n",
    "        if user_query == \"jarvis\":\n",
    "            print(\"Manual: \\nFAQ: Show original data \\nBOW: Bag of Words \\nCount: Show the count\")\n",
    "        elif user_query == \"FAQ\":\n",
    "            print(data)\n",
    "        elif user_query == \"show\":\n",
    "            for faq in faq_corpus:\n",
    "                print(faq_corpus.index(faq))\n",
    "                faq.print()\n",
    "        elif user_query == \"BOW\":\n",
    "            for faq in faq_corpus:\n",
    "                print(faq_corpus.index(faq))\n",
    "                faq.print_bag()\n",
    "        elif user_query == \"Count\":\n",
    "            for faq in faq_corpus:\n",
    "                print(faq_corpus.index(faq))\n",
    "                faq.print_counter()\n",
    "        else:\n",
    "            # Remove the unwanted spaces, tabs and new line character\n",
    "            user_input = user_query.strip()\n",
    "\n",
    "            # Split the strings (words) into tokens based on whitespace and punctuation\n",
    "            # Also called as Bag of Words\n",
    "            user_bag = word_tokenize(user_input)\n",
    "            user_counter = Counter(user_bag)\n",
    "            \n",
    "            cos_counter = {}\n",
    "            for faq in faq_corpus:\n",
    "                cos = cosine_similarity(user_counter, Counter(faq.bow))\n",
    "                cos_counter[faq_corpus.index(faq)] = cos\n",
    "\n",
    "            show_count = 0\n",
    "            for index in sorted(cos_counter, key=cos_counter.get, reverse=True):\n",
    "                if cos_counter[index] > 0 and show_count < num_of_results:\n",
    "                    print(\"*****FAQ Index: \", index, \"\\t*****Cosine Similarity: \", cos_counter[index])\n",
    "                    faq_corpus[index].print()\n",
    "                    print()\n",
    "                    show_count += 1\n",
    "        x-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Run Task2(1) for Task2\n",
    "\n",
    "# Run Task4(1) for Task4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}